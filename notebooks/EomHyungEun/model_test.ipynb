{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../../data/processed/train.csv\")\n",
    "test_df = pd.read_csv(\"../../data/processed/test.csv\")\n",
    "\n",
    "display(train_df.describe())\n",
    "display(train_df.info())\n",
    "display(train_df.head())\n",
    "\n",
    "display(test_df.describe())\n",
    "display(test_df.info())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(\"Revenue\", axis=1)\n",
    "y_train = train_df[\"Revenue\"]\n",
    "\n",
    "X_test = test_df.drop(\"Revenue\", axis=1)\n",
    "y_test = test_df[\"Revenue\"]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ucimlrepo pandas scikit-learn numpy joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_recall_curve,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "MODEL_PATH = \"online_shoppers_lr_robust_onehot.joblib\"\n",
    "\n",
    "\n",
    "# 2) Define columns (UCI schema 기준)\n",
    "numeric_cols = [\n",
    "    \"Administrative\", \"Administrative_Duration\",\n",
    "    \"Informational\", \"Informational_Duration\",\n",
    "    \"ProductRelated\", \"ProductRelated_Duration\",\n",
    "    \"BounceRates\", \"ExitRates\",\n",
    "    \"PageValues\", \"SpecialDay\",\n",
    "]\n",
    "categorical_cols = [\n",
    "    \"Month\", \"OperatingSystems\", \"Browser\", \"Region\",\n",
    "    \"TrafficType\", \"VisitorType\", \"Weekend\",\n",
    "]\n",
    "\n",
    "# 4) Preprocess (RobustScaler + OneHot)\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# 5) Model with class_weight for imbalance\n",
    "clf = LogisticRegression(\n",
    "    max_iter=5000,\n",
    "    solver=\"saga\",           # sparse one-hot에 적합\n",
    "    class_weight=\"balanced\", # 불균형 가중치\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", clf),\n",
    "])\n",
    "\n",
    "# 6) Fit\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7) Predict / Evaluate\n",
    "proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "pred_default = (proba >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "prauc = average_precision_score(y_test, proba)  # PR-AUC\n",
    "f1 = f1_score(y_test, pred_default)\n",
    "\n",
    "print(\"=== Evaluation (threshold=0.5) ===\")\n",
    "print(f\"ROC-AUC : {roc:.4f}\")\n",
    "print(f\"PR-AUC  : {prauc:.4f}\")\n",
    "print(f\"F1      : {f1:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred_default))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred_default, digits=4))\n",
    "\n",
    "# (선택) PR 커브 기반으로 F1 최대가 되는 임계값도 같이 출력\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_thr = thr[best_idx - 1] if best_idx > 0 else 0.5  # thr 길이가 하나 짧음\n",
    "print(f\"\\nBest-F1 threshold (from PR curve): {best_thr:.4f}, F1={f1s[best_idx]:.4f}\")\n",
    "\n",
    "# 8) Extract/Save model (전처리+모델 통째로 저장)\n",
    "joblib.dump(pipeline, MODEL_PATH)\n",
    "print(f\"\\nSaved pipeline to: {MODEL_PATH}\")\n",
    "\n",
    "# 9) (검증) 로드 후 동일 예측 되는지 간단 체크\n",
    "loaded = joblib.load(MODEL_PATH)\n",
    "proba2 = loaded.predict_proba(X_test)[:, 1]\n",
    "print(\"Reload check (max abs diff in proba):\", float(np.max(np.abs(proba - proba2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ===== 전제: 이미 아래 변수들이 존재한다고 가정 =====\n",
    "# pipeline  : 학습된 Pipeline (preprocess + model)\n",
    "# X_test    : 테스트 피처 (DataFrame)\n",
    "# y_test    : 테스트 라벨 (0/1)\n",
    "# proba     : pipeline.predict_proba(X_test)[:, 1]\n",
    "# pred_default : (proba >= 0.5).astype(int)\n",
    "\n",
    "# 1) ROC Curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "RocCurveDisplay.from_predictions(y_test, proba)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Precision-Recall Curve (불균형에서 중요)\n",
    "plt.figure(figsize=(6, 5))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, proba)\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Confusion Matrix (threshold=0.5)\n",
    "cm = confusion_matrix(y_test, pred_default)\n",
    "plt.figure(figsize=(5.5, 4.5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (threshold=0.5)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Calibration Curve (확률이 얼마나 믿을만한지)\n",
    "prob_true, prob_pred = calibration_curve(y_test, proba, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Predicted probability distribution (구매/비구매 확률 분포가 갈라지는지)\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "sns.kdeplot(proba[y_test == 0], label=\"Non-purchase (y=0)\", fill=True, alpha=0.35)\n",
    "sns.kdeplot(proba[y_test == 1], label=\"Purchase (y=1)\", fill=True, alpha=0.35)\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.title(\"Predicted Probability Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Permutation Importance (Pipeline에서도 가능)\n",
    "#    - OneHot까지 포함한 전체 피처 중요도는 컬럼이 너무 많아질 수 있어서\n",
    "#      여기서는 “원본 컬럼 단위”로 보고 싶다면 별도 설계가 필요함.\n",
    "#    - 대신: transformed feature 기준 top-k를 뽑아 보여줌.\n",
    "\n",
    "# (주의) Pipeline에서 preprocess 결과는 sparse일 수 있음. permutation_importance는 동작하지만 느릴 수 있어.\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring=\"average_precision\"  # PR-AUC 기반 중요도\n",
    ")\n",
    "\n",
    "importances = result.importances_mean\n",
    "idx = np.argsort(importances)[::-1]\n",
    "\n",
    "# transformed feature names 얻기\n",
    "pre = pipeline.named_steps[\"preprocess\"]\n",
    "feature_names = pre.get_feature_names_out()\n",
    "\n",
    "top_k = 25\n",
    "top_idx = idx[:top_k]\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.barplot(x=importances[top_idx], y=np.array(feature_names)[top_idx], orient=\"h\")\n",
    "plt.title(f\"Permutation Importance (Top {top_k}, scoring=PR-AUC)\")\n",
    "plt.xlabel(\"Mean importance decrease\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn joblib matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"   # 네 csv에서 타깃 컬럼명\n",
    "MODEL_PATH = \"model_stacking_robust_onehot.joblib\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "if TARGET_COL not in train_df.columns:\n",
    "    raise ValueError(f\"TARGET_COL='{TARGET_COL}' not found in train.csv columns: {list(train_df.columns)}\")\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)  # True/False or 0/1 모두 대응(대부분 int로 변환 가능)\n",
    "\n",
    "# test에도 라벨이 있다면 평가 가능, 없다면 예측만 가능하게 처리\n",
    "has_test_label = TARGET_COL in test_df.columns\n",
    "X_test = test_df.drop(columns=[TARGET_COL]) if has_test_label else test_df.copy()\n",
    "y_test = test_df[TARGET_COL].astype(int) if has_test_label else None\n",
    "\n",
    "# =========================\n",
    "# Column typing (자동 추정)\n",
    "# =========================\n",
    "# - 숫자형: number dtype\n",
    "# - 범주형: object/bool/category\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "# (안전) Month가 문자열인데 숫자로 읽혔거나, 반대일 수도 있음 → 필요하면 여기서 수동 지정\n",
    "# print(\"num_cols:\", num_cols)\n",
    "# print(\"cat_cols:\", cat_cols)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Models (불균형 weight 적용)\n",
    "# =========================\n",
    "lr = LogisticRegression(\n",
    "    max_iter=5000, solver=\"saga\", class_weight=\"balanced\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=600,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "meta = LogisticRegression(\n",
    "    max_iter=5000, solver=\"saga\", class_weight=\"balanced\",\n",
    "    n_jobs=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=[(\"lr\", lr), (\"rf\", rf), (\"et\", et)],\n",
    "    final_estimator=meta,\n",
    "    stack_method=\"predict_proba\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", stack),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 1) OOF 확률로 threshold 튜닝 (train만 사용, 추가 split 없음)\n",
    "# =========================\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_proba = np.zeros(len(X_train), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_tr, y_tr = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    X_va, y_va = X_train.iloc[va_idx], y_train.iloc[va_idx]\n",
    "\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    oof_proba[va_idx] = pipeline.predict_proba(X_va)[:, 1]\n",
    "    print(f\"[Fold {fold}] done\")\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_thr = thr[best_idx - 1] if best_idx > 0 else 0.5\n",
    "print(\"\\n[OOF] Best threshold:\", float(best_thr), \"Best F1:\", float(f1s[best_idx]))\n",
    "\n",
    "# =========================\n",
    "# 2) train 전체로 재학습\n",
    "# =========================\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# =========================\n",
    "# 3) test 평가(라벨 있을 때)\n",
    "# =========================\n",
    "test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "if has_test_label:\n",
    "    print(\"\\n=== TEST Metrics (using OOF-tuned threshold) ===\")\n",
    "    print(\"ROC-AUC :\", roc_auc_score(y_test, test_proba))\n",
    "    print(\"PR-AUC  :\", average_precision_score(y_test, test_proba))\n",
    "    print(\"F1      :\", f1_score(y_test, test_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, test_pred))\n",
    "    print(\"\\nReport:\\n\", classification_report(y_test, test_pred, digits=4))\n",
    "else:\n",
    "    print(\"\\n(test.csv에 Revenue가 없어서 평가 대신 예측만 생성합니다.)\")\n",
    "\n",
    "# =========================\n",
    "# 4) 모델 추출/저장 (pipeline + threshold 같이 저장)\n",
    "# =========================\n",
    "artifact = {\n",
    "    \"pipeline\": pipeline,\n",
    "    \"best_threshold\": float(best_thr),\n",
    "    \"num_cols\": num_cols,\n",
    "    \"cat_cols\": cat_cols\n",
    "}\n",
    "joblib.dump(artifact, MODEL_PATH)\n",
    "print(\"\\nSaved:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 전제: y_test, test_proba, test_pred 가 존재해야 함 (test에 Revenue 있을 때)\n",
    "plt.figure(figsize=(6,5))\n",
    "RocCurveDisplay.from_predictions(y_test, test_proba)\n",
    "plt.title(\"ROC Curve (TEST)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "PrecisionRecallDisplay.from_predictions(y_test, test_proba)\n",
    "plt.title(\"Precision-Recall Curve (TEST)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "plt.figure(figsize=(5.5,4.5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(f\"Confusion Matrix (TEST, thr={artifact['best_threshold']:.3f})\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration curve (확률 신뢰도)\n",
    "prob_true, prob_pred = calibration_curve(y_test, test_proba, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfect\")\n",
    "plt.title(\"Calibration Curve (TEST)\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 확률 분포 (구매/비구매 분리 정도)\n",
    "plt.figure(figsize=(7,4.5))\n",
    "sns.kdeplot(test_proba[y_test==0], label=\"y=0\", fill=True, alpha=0.35)\n",
    "sns.kdeplot(test_proba[y_test==1], label=\"y=1\", fill=True, alpha=0.35)\n",
    "plt.title(\"Predicted Probability Distribution (TEST)\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "MODEL_PATH = \"brf_robust_onehot.joblib\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "if TARGET_COL not in train_df.columns:\n",
    "    raise ValueError(f\"'{TARGET_COL}' not in train.csv\")\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "\n",
    "has_test_label = TARGET_COL in test_df.columns\n",
    "X_test = test_df.drop(columns=[TARGET_COL]) if has_test_label else test_df.copy()\n",
    "y_test = test_df[TARGET_COL].astype(int) if has_test_label else None\n",
    "\n",
    "# =========================\n",
    "# Column typing (자동 추정)\n",
    "# =========================\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Model: Balanced Random Forest\n",
    "# =========================\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    sampling_strategy=\"auto\",   # 각 트리 학습 시 클래스 균형 샘플링\n",
    "    replacement=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", brf),\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 1) OOF로 threshold 튜닝 (추가 split 없음)\n",
    "# =========================\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_proba = np.zeros(len(X_train), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_tr, y_tr = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    X_va, y_va = X_train.iloc[va_idx], y_train.iloc[va_idx]\n",
    "\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    oof_proba[va_idx] = pipeline.predict_proba(X_va)[:, 1]\n",
    "    print(f\"[Fold {fold}] done\")\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_thr = thr[best_idx - 1] if best_idx > 0 else 0.5\n",
    "\n",
    "print(\"\\n[OOF] Best threshold:\", float(best_thr), \"Best F1:\", float(f1s[best_idx]))\n",
    "\n",
    "# =========================\n",
    "# 2) train 전체로 재학습\n",
    "# =========================\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# =========================\n",
    "# 3) test 평가(라벨 있으면)\n",
    "# =========================\n",
    "test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "if has_test_label:\n",
    "    print(\"\\n=== TEST Metrics (BRF, OOF-threshold) ===\")\n",
    "    print(\"ROC-AUC :\", roc_auc_score(y_test, test_proba))\n",
    "    print(\"PR-AUC  :\", average_precision_score(y_test, test_proba))\n",
    "    print(\"F1      :\", f1_score(y_test, test_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, test_pred))\n",
    "    print(\"\\nReport:\\n\", classification_report(y_test, test_pred, digits=4))\n",
    "else:\n",
    "    print(\"\\n(test.csv에 Revenue가 없어 평가 대신 예측만 생성합니다.)\")\n",
    "\n",
    "# =========================\n",
    "# 4) 모델 추출/저장 (pipeline + threshold)\n",
    "# =========================\n",
    "artifact = {\n",
    "    \"pipeline\": pipeline,\n",
    "    \"best_threshold\": float(best_thr),\n",
    "    \"num_cols\": num_cols,\n",
    "    \"cat_cols\": cat_cols\n",
    "}\n",
    "joblib.dump(artifact, MODEL_PATH)\n",
    "print(\"\\nSaved:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "MODEL_PATH = \"easyensemble_robust_onehot.joblib\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "\n",
    "has_test_label = TARGET_COL in test_df.columns\n",
    "X_test = test_df.drop(columns=[TARGET_COL]) if has_test_label else test_df.copy()\n",
    "y_test = test_df[TARGET_COL].astype(int) if has_test_label else None\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# EasyEnsemble은 언더샘플링된 여러 서브셋을 만들고, 각 서브셋에 약한 분류기를 학습한 뒤 앙상블\n",
    "base = LogisticRegression(max_iter=5000, solver=\"saga\", class_weight=None, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "eec = EasyEnsembleClassifier(\n",
    "    n_estimators=30,          # 서브셋 개수(늘리면 더 안정적/느려짐)\n",
    "    estimator=base,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", eec),\n",
    "])\n",
    "\n",
    "# OOF threshold 튜닝\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_proba = np.zeros(len(X_train), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_tr, y_tr = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    X_va, y_va = X_train.iloc[va_idx], y_train.iloc[va_idx]\n",
    "\n",
    "    pipeline.fit(X_tr, y_tr)\n",
    "    oof_proba[va_idx] = pipeline.predict_proba(X_va)[:, 1]\n",
    "    print(f\"[Fold {fold}] done\")\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_thr = thr[best_idx - 1] if best_idx > 0 else 0.5\n",
    "\n",
    "print(\"\\n[OOF] Best threshold:\", float(best_thr), \"Best F1:\", float(f1s[best_idx]))\n",
    "\n",
    "# train 전체 재학습\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# test 평가\n",
    "test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "if has_test_label:\n",
    "    print(\"\\n=== TEST Metrics (EasyEnsemble, OOF-threshold) ===\")\n",
    "    print(\"ROC-AUC :\", roc_auc_score(y_test, test_proba))\n",
    "    print(\"PR-AUC  :\", average_precision_score(y_test, test_proba))\n",
    "    print(\"F1      :\", f1_score(y_test, test_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, test_pred))\n",
    "    print(\"\\nReport:\\n\", classification_report(y_test, test_pred, digits=4))\n",
    "\n",
    "artifact = {\"pipeline\": pipeline, \"best_threshold\": float(best_thr), \"num_cols\": num_cols, \"cat_cols\": cat_cols}\n",
    "joblib.dump(artifact, MODEL_PATH)\n",
    "print(\"\\nSaved:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../../data/processed/train.csv\")\n",
    "test_df  = pd.read_csv(\"../../data/processed/test.csv\")\n",
    "\n",
    "for name, df in [(\"train\", train_df), (\"test\", test_df)]:\n",
    "    if \"Revenue\" in df.columns:\n",
    "        vc = df[\"Revenue\"].astype(int).value_counts().sort_index()\n",
    "        pos = int(vc.get(1, 0)); neg = int(vc.get(0, 0))\n",
    "        print(f\"{name}: pos={pos}, neg={neg}, pos_rate={pos/(pos+neg):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL])\n",
    "y_test = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "# 자동 컬럼 타입\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def oof_best_threshold(model_pipeline, X, y, n_splits=5, seed=42):\n",
    "    \"\"\"train만으로 OOF 확률 생성 -> F1 최대 threshold 반환\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        model_pipeline.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "        oof[va_idx] = model_pipeline.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y, oof)\n",
    "    f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "    best_idx = np.nanargmax(f1s)\n",
    "    best_thr = thr[best_idx - 1] if best_idx > 0 else 0.5\n",
    "    return float(best_thr), float(f1s[best_idx])\n",
    "\n",
    "def fit_eval(name, base_model):\n",
    "    pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", base_model)])\n",
    "\n",
    "    best_thr, best_oof_f1 = oof_best_threshold(pipe, X_train, y_train, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "\n",
    "    # train 전체로 재학습\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    test_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "\n",
    "    out = {\n",
    "        \"name\": name,\n",
    "        \"oof_best_f1\": best_oof_f1,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"test_f1\": float(f1_score(y_test, test_pred)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, test_proba)),\n",
    "        \"test_pr_auc\": float(average_precision_score(y_test, test_proba)),\n",
    "        \"pipe\": pipe\n",
    "    }\n",
    "    return out, test_pred, test_proba\n",
    "\n",
    "models = [\n",
    "    (\"LR_balanced\", LogisticRegression(max_iter=5000, solver=\"saga\", class_weight=\"balanced\", n_jobs=-1, random_state=RANDOM_STATE)),\n",
    "    (\"BalancedRF\", BalancedRandomForestClassifier(n_estimators=800, n_jobs=-1, random_state=RANDOM_STATE)),\n",
    "    (\"EasyEnsemble\", EasyEnsembleClassifier(\n",
    "        n_estimators=30,\n",
    "        estimator=LogisticRegression(max_iter=5000, solver=\"saga\", n_jobs=-1, random_state=RANDOM_STATE),\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "    (\"ExtraTrees_balanced\", ExtraTreesClassifier(n_estimators=800, class_weight=\"balanced\", n_jobs=-1, random_state=RANDOM_STATE)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "best = None\n",
    "\n",
    "for name, m in models:\n",
    "    res, test_pred, test_proba = fit_eval(name, m)\n",
    "    results.append(res)\n",
    "    print(f\"\\n[{name}] OOF_best_F1={res['oof_best_f1']:.4f}, best_thr={res['best_thr']:.4f} | \"\n",
    "          f\"TEST F1={res['test_f1']:.4f}, PR-AUC={res['test_pr_auc']:.4f}, ROC-AUC={res['test_roc_auc']:.4f}\")\n",
    "\n",
    "    if best is None or res[\"test_f1\"] > best[\"test_f1\"]:\n",
    "        best = res\n",
    "\n",
    "print(\"\\n=== BEST by TEST F1 ===\")\n",
    "print(best[\"name\"], \"TEST F1=\", best[\"test_f1\"], \"best_thr=\", best[\"best_thr\"])\n",
    "\n",
    "# 최종 상세 리포트(베스트 모델로)\n",
    "best_pipe = best[\"pipe\"]\n",
    "best_thr  = best[\"best_thr\"]\n",
    "best_test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "best_test_pred  = (best_test_proba >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, best_test_pred))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, best_test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    132\u001b[39m best = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, pipe \u001b[38;5;129;01min\u001b[39;00m candidates:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     res = \u001b[43mfit_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     results.append(res)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] OOF_best_F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33moof_best_f1\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, thr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mbest_thr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTEST F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mtest_f1\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, PR-AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mtest_pr_auc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, ROC-AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[33m'\u001b[39m\u001b[33mtest_roc_auc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mfit_eval\u001b[39m\u001b[34m(name, pipe)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_eval\u001b[39m(name, pipe):\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# 1) OOF로 best threshold\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     thr, oof_f1 = \u001b[43moof_best_threshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# 2) train 전체로 재학습\u001b[39;00m\n\u001b[32m     70\u001b[39m     pipe.fit(X_train, y_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36moof_best_threshold\u001b[39m\u001b[34m(pipeline, X, y, n_splits, seed)\u001b[39m\n\u001b[32m     53\u001b[39m oof = np.zeros(\u001b[38;5;28mlen\u001b[39m(X), dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tr_idx, va_idx \u001b[38;5;129;01min\u001b[39;00m skf.split(X, y):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     oof[va_idx] = pipeline.predict_proba(X.iloc[va_idx])[:, \u001b[32m1\u001b[39m]\n\u001b[32m     59\u001b[39m prec, rec, thr = precision_recall_curve(y, oof)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/imblearn/ensemble/_easy_ensemble.py:264\u001b[39m, in \u001b[36mEasyEnsembleClassifier.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_params()\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# overwrite the base class method by disallowing `sample_weight`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/ensemble/_bagging.py:389\u001b[39m, in \u001b[36mBaseBagging.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[32m    379\u001b[39m X, y = validate_data(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    381\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    386\u001b[39m     multi_output=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    387\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/imblearn/ensemble/_easy_ensemble.py:270\u001b[39m, in \u001b[36mEasyEnsembleClassifier._fit\u001b[39m\u001b[34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[39m\n\u001b[32m    267\u001b[39m check_target_type(y)\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# RandomUnderSampler is not supporting sample_weight. We need to pass\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# None.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/ensemble/_bagging.py:547\u001b[39m, in \u001b[36mBaseBagging._fit\u001b[39m\u001b[34m(self, X, y, max_samples, max_depth, check_input, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    544\u001b[39m seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n\u001b[32m    545\u001b[39m \u001b[38;5;28mself\u001b[39m._seeds = seeds\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m all_results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ += \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    566\u001b[39m     itertools.chain.from_iterable(t[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[32m    567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL])\n",
    "y_test = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "# ---- 컬럼 자동 분리 (원하면 여기서 수동 지정 가능)\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def oof_best_threshold(pipeline, X, y, n_splits=5, seed=42):\n",
    "    \"\"\"train에서만 OOF 확률 생성 -> F1 최대 threshold 반환\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        pipeline.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "        oof[va_idx] = pipeline.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y, oof)\n",
    "    f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1s))\n",
    "    best_thr = float(thr[best_idx - 1]) if best_idx > 0 else 0.5\n",
    "    return best_thr, float(f1s[best_idx])\n",
    "\n",
    "def fit_eval(name, pipe):\n",
    "    # 1) OOF로 best threshold\n",
    "    thr, oof_f1 = oof_best_threshold(pipe, X_train, y_train, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "\n",
    "    # 2) train 전체로 재학습\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # 3) test 평가\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= thr).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"oof_best_f1\": oof_f1,\n",
    "        \"best_thr\": thr,\n",
    "        \"test_f1\": float(f1_score(y_test, pred)),\n",
    "        \"test_pr_auc\": float(average_precision_score(y_test, proba)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, proba)),\n",
    "        \"pipe\": pipe,\n",
    "        \"test_pred\": pred,\n",
    "        \"test_proba\": proba\n",
    "    }\n",
    "\n",
    "# ---- 후보 모델들 (F1 잘 나오는 쪽 위주)\n",
    "candidates = []\n",
    "\n",
    "# 1) ExtraTrees (강추: tabular에서 F1 잘 나오기 쉬움)\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=1200,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "candidates.append((\"ExtraTrees_balanced\",\n",
    "                   Pipeline([(\"preprocess\", preprocess), (\"model\", et)])))\n",
    "\n",
    "# 2) Balanced Random Forest\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=900,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "candidates.append((\"BalancedRF\",\n",
    "                   Pipeline([(\"preprocess\", preprocess), (\"model\", brf)])))\n",
    "\n",
    "# 3) EasyEnsemble (양성이 적을 때 특히 안정적인 경우 많음)\n",
    "base_lr = LogisticRegression(max_iter=5000, solver=\"saga\", n_jobs=-1, random_state=RANDOM_STATE)\n",
    "eec = EasyEnsembleClassifier(\n",
    "    n_estimators=40,\n",
    "    estimator=base_lr,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "candidates.append((\"EasyEnsemble\",\n",
    "                   Pipeline([(\"preprocess\", preprocess), (\"model\", eec)])))\n",
    "\n",
    "# 4) (옵션) RandomOverSampler + LR (가중치 대신 표본 보강)\n",
    "ros_lr = LogisticRegression(max_iter=5000, solver=\"saga\", n_jobs=-1, random_state=RANDOM_STATE)\n",
    "candidates.append((\"ROS+LR\",\n",
    "                   ImbPipeline([(\"preprocess\", preprocess),\n",
    "                               (\"ros\", RandomOverSampler(random_state=RANDOM_STATE)),\n",
    "                               (\"model\", ros_lr)])))\n",
    "\n",
    "# ---- 실행\n",
    "results = []\n",
    "best = None\n",
    "\n",
    "for name, pipe in candidates:\n",
    "    res = fit_eval(name, pipe)\n",
    "    results.append(res)\n",
    "    print(f\"\\n[{name}] OOF_best_F1={res['oof_best_f1']:.4f}, thr={res['best_thr']:.4f} | \"\n",
    "          f\"TEST F1={res['test_f1']:.4f}, PR-AUC={res['test_pr_auc']:.4f}, ROC-AUC={res['test_roc_auc']:.4f}\")\n",
    "\n",
    "    if best is None or res[\"test_f1\"] > best[\"test_f1\"]:\n",
    "        best = res\n",
    "\n",
    "print(\"\\n=== BEST (by TEST F1) ===\")\n",
    "print(best[\"name\"], \"TEST F1=\", best[\"test_f1\"], \"thr=\", best[\"best_thr\"])\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, best[\"test_pred\"]))\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, best[\"test_pred\"], digits=4))\n",
    "\n",
    "# ---- 저장(모델 추출)\n",
    "artifact = {\n",
    "    \"pipeline\": best[\"pipe\"],\n",
    "    \"best_threshold\": float(best[\"best_thr\"]),\n",
    "    \"model_name\": best[\"name\"],\n",
    "    \"num_cols\": num_cols,\n",
    "    \"cat_cols\": cat_cols\n",
    "}\n",
    "joblib.dump(artifact, \"best_f1_model.joblib\")\n",
    "print(\"\\nSaved: best_f1_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "X_test  = test_df.drop(columns=[TARGET_COL])\n",
    "y_test  = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "# 자동 컬럼 타입\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def oof_proba_for_pipe(pipe, X, y, n_splits=5, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        pipe.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "        oof[va_idx] = pipe.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "    return oof\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, proba)\n",
    "    f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1s))\n",
    "    best_thr = float(thr[best_idx - 1]) if best_idx > 0 else 0.5\n",
    "    return best_thr, float(f1s[best_idx])\n",
    "\n",
    "# ---- 튜닝 그리드 (너무 크면 오래 걸리니 \"F1 잘 움직이는 축\"만 잡음)\n",
    "grid = []\n",
    "for max_depth in [None, 8, 12, 16]:\n",
    "    for min_leaf in [1, 2, 4, 8]:\n",
    "        for max_features in [\"sqrt\", 0.5, 0.8]:\n",
    "            grid.append({\n",
    "                \"n_estimators\": 1200,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"min_samples_leaf\": min_leaf,\n",
    "                \"min_samples_split\": 2,\n",
    "                \"max_features\": max_features,\n",
    "                \"sampling_strategy\": \"auto\",\n",
    "                \"replacement\": False,\n",
    "            })\n",
    "\n",
    "best = None\n",
    "\n",
    "for i, params in enumerate(grid, 1):\n",
    "    brf = BalancedRandomForestClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        max_features=params[\"max_features\"],\n",
    "        sampling_strategy=params[\"sampling_strategy\"],\n",
    "        replacement=params[\"replacement\"],\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", brf)])\n",
    "\n",
    "    oof = oof_proba_for_pipe(pipe, X_train, y_train, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "    thr, oof_f1 = best_f1_threshold(y_train, oof)\n",
    "\n",
    "    # train 전체 재학습 후 test 평가\n",
    "    pipe.fit(X_train, y_train)\n",
    "    test_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= thr).astype(int)\n",
    "    test_f1 = float(f1_score(y_test, test_pred))\n",
    "\n",
    "    if best is None or test_f1 > best[\"test_f1\"]:\n",
    "        best = {\n",
    "            \"params\": params,\n",
    "            \"thr\": thr,\n",
    "            \"oof_f1\": oof_f1,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"test_pr_auc\": float(average_precision_score(y_test, test_proba)),\n",
    "            \"test_roc_auc\": float(roc_auc_score(y_test, test_proba)),\n",
    "        }\n",
    "\n",
    "    print(f\"[{i}/{len(grid)}] depth={params['max_depth']}, leaf={params['min_samples_leaf']}, \"\n",
    "          f\"max_feat={params['max_features']} | OOF_F1={oof_f1:.4f} thr={thr:.3f} | TEST_F1={test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== BEST CONFIG (by TEST F1) ===\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy scikit-learn imbalanced-learn joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "OUT_PATH   = \"artifacts/best_pr_auc_balancedrf.joblib\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "X_test  = test_df.drop(columns=[TARGET_COL])\n",
    "y_test  = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "base = BalancedRandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", base),\n",
    "])\n",
    "\n",
    "# PR-AUC 최적화용 탐색 공간 (너무 크게 잡지 말고, 영향 큰 축 위주)\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": [300, 500, 800, 1200],\n",
    "    \"model__max_depth\": [None, 8, 12, 16, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4, 8, 12],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__max_features\": [\"sqrt\", 0.3, 0.5, 0.8],\n",
    "    \"model__sampling_strategy\": [\"auto\", 0.5, 0.7, 1.0],\n",
    "    \"model__replacement\": [False, True],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,  # 시간 여유 있으면 80~120까지 올리면 더 좋아짐\n",
    "    scoring=\"average_precision\",  # PR-AUC\n",
    "    cv=cv,\n",
    "    refit=True,  # best 모델로 train 전체 재학습\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"\\nBest CV PR-AUC:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "# test 평가(참고용)\n",
    "test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nTEST PR-AUC :\", average_precision_score(y_test, test_proba))\n",
    "print(\"TEST ROC-AUC:\", roc_auc_score(y_test, test_proba))\n",
    "\n",
    "# 저장 (압축 권장)\n",
    "artifact = {\n",
    "    \"pipeline\": best_model,\n",
    "    \"best_params\": search.best_params_,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"num_cols\": num_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"target_col\": TARGET_COL,\n",
    "}\n",
    "Path(OUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(artifact, OUT_PATH, compress=(\"xz\", 3))\n",
    "print(\"\\nSaved:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 📊 시각화용 공통 설정\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# RandomizedSearchCV 결과를 DataFrame으로 변환\n",
    "cv_results = pd.DataFrame(search.cv_results_)\n",
    "cv_results[\"mean_pr_auc\"] = cv_results[\"mean_test_score\"]  # 이름을 더 직관적으로\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. CV vs Test 성능 요약 막대그래프\n",
    "# =========================\n",
    "\n",
    "cv_pr_auc = float(search.best_score_)                 # = Best CV PR-AUC\n",
    "test_pr_auc = float(average_precision_score(y_test, test_proba))\n",
    "test_roc_auc = float(roc_auc_score(y_test, test_proba))\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"CV PR-AUC\", \"Test PR-AUC\", \"Test ROC-AUC\"],\n",
    "        \"value\": [cv_pr_auc, test_pr_auc, test_roc_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.barplot(data=metrics_df, x=\"metric\", y=\"value\", palette=\"Blues_d\", legend=False, hue='metric')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "plt.ylim(0, 1.0)\n",
    "# \"Best PR-AUC 모델 성능 요약 (CV vs Test)\n",
    "plt.title(\"Best PR-AUC Model Performance Summary (CV vs. Test)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# -> PPT에 \"성능 요약\" 슬라이드로 바로 캡처해서 사용\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. 하이퍼파라미터별 PR-AUC 영향 시각화\n",
    "#    (각 파라미터 vs mean PR-AUC)\n",
    "# =========================\n",
    "\n",
    "params_to_plot = [\n",
    "    \"param_model__n_estimators\",\n",
    "    \"param_model__max_depth\",\n",
    "    \"param_model__min_samples_leaf\",\n",
    "    \"param_model__min_samples_split\",\n",
    "    \"param_model__max_features\",\n",
    "    \"param_model__sampling_strategy\",\n",
    "    \"param_model__replacement\",\n",
    "]\n",
    "\n",
    "def plot_param_effects(cv_results, params, score_col=\"mean_pr_auc\"):\n",
    "    n = len(params)\n",
    "    ncols = 2\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 4 * nrows))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for ax, param in zip(axes, params):\n",
    "        if param not in cv_results.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        # 값 목록\n",
    "        values = cv_results[param].dropna().unique()\n",
    "\n",
    "        # 숫자인지/문자인지 구분\n",
    "        def is_numeric(v):\n",
    "            return isinstance(v, (int, float, np.number))\n",
    "\n",
    "        if all(is_numeric(v) for v in values):\n",
    "            # 숫자형 파라미터: 값별 평균 PR-AUC를 선그래프로\n",
    "            tmp = (\n",
    "                cv_results.groupby(param)[score_col]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .sort_values(param)\n",
    "            )\n",
    "            sns.lineplot(data=tmp, x=param, y=score_col, marker=\"o\", ax=ax)\n",
    "        else:\n",
    "            # 범주형 파라미터: 박스플롯/포인트플롯\n",
    "            tmp = cv_results[[param, score_col]].copy()\n",
    "            tmp[param] = tmp[param].astype(str)\n",
    "            sns.boxplot(data=tmp, x=param, y=score_col, ax=ax)\n",
    "            ax.tick_params(\"x\", labelrotation=30)\n",
    "\n",
    "        ax.set_title(param.replace(\"param_model__\", \"model__\"))\n",
    "        ax.set_ylabel(\"mean CV PR-AUC\")\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "    # 남는 subplot 숨기기\n",
    "    for i in range(len(params), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    # Hyperparameter 별 PR-AUC 수치 변화\n",
    "    plt.suptitle(\"Changes in CV PR-AUC by hyperparameter\", y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_param_effects(cv_results, params_to_plot)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Best vs 다른 후보들 비교 (Ranking Plot)\n",
    "# =========================\n",
    "\n",
    "# 상위 k개만 보기 (노이즈 줄이기용)\n",
    "k = 20\n",
    "topk = (\n",
    "    cv_results.sort_values(\"mean_pr_auc\", ascending=False)\n",
    "    .head(k)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.lineplot(\n",
    "    data=topk,\n",
    "    x=topk.index + 1,\n",
    "    y=\"mean_pr_auc\",\n",
    "    marker=\"o\",\n",
    "    sort=False,\n",
    ")\n",
    "\n",
    "# 무작위 탐색 상위 {k}개 구성의 CV PR-AUC\n",
    "plt.title(f\"CV PR-AUC of top {k} configurations from random search\")\n",
    "plt.xlabel(\"Rank (1 = best)\")\n",
    "plt.ylabel(\"mean CV PR-AUC\")\n",
    "plt.xticks(range(1, k + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 튜닝 그리드 (너무 크면 오래 걸리니 \"F1 잘 움직이는 축\"만 잡음)\n",
    "grid = []\n",
    "for max_depth in [None, 8, 12, 16]:\n",
    "    for min_leaf in [1, 2, 4, 8]:\n",
    "        for max_features in [\"sqrt\", 0.5, 0.8]:\n",
    "            grid.append({\n",
    "                \"n_estimators\": 1200,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"min_samples_leaf\": min_leaf,\n",
    "                \"min_samples_split\": 2,\n",
    "                \"max_features\": max_features,\n",
    "                \"sampling_strategy\": \"auto\",\n",
    "                \"replacement\": False,\n",
    "            })\n",
    "\n",
    "best = None\n",
    "results = []   # 👈 모든 조합의 결과를 담을 리스트 추가\n",
    "\n",
    "for i, params in enumerate(grid, 1):\n",
    "    brf = BalancedRandomForestClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        max_features=params[\"max_features\"],\n",
    "        sampling_strategy=params[\"sampling_strategy\"],\n",
    "        replacement=params[\"replacement\"],\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", brf)])\n",
    "\n",
    "    oof = oof_proba_for_pipe(pipe, X_train, y_train, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "    thr, oof_f1 = best_f1_threshold(y_train, oof)\n",
    "\n",
    "    # train 전체 재학습 후 test 평가\n",
    "    pipe.fit(X_train, y_train)\n",
    "    test_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= thr).astype(int)\n",
    "    test_f1 = float(f1_score(y_test, test_pred))\n",
    "    test_pr_auc = float(average_precision_score(y_test, test_proba))\n",
    "    test_roc_auc = float(roc_auc_score(y_test, test_proba))\n",
    "\n",
    "    # 👇 여기서 각 조합의 결과를 results에 저장\n",
    "    results.append({\n",
    "        **params,                # 각 하이퍼파라미터 펼쳐서 저장\n",
    "        \"thr\": thr,\n",
    "        \"oof_f1\": oof_f1,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_pr_auc\": test_pr_auc,\n",
    "        \"test_roc_auc\": test_roc_auc,\n",
    "    })\n",
    "\n",
    "    if best is None or test_f1 > best[\"test_f1\"]:\n",
    "        best = {\n",
    "            \"params\": params,\n",
    "            \"thr\": thr,\n",
    "            \"oof_f1\": oof_f1,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"test_pr_auc\": test_pr_auc,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "        }\n",
    "\n",
    "    print(f\"[{i}/{len(grid)}] depth={params['max_depth']}, leaf={params['min_samples_leaf']}, \"\n",
    "          f\"max_feat={params['max_features']} | OOF_F1={oof_f1:.4f} thr={thr:.3f} | TEST_F1={test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== BEST CONFIG (by TEST F1) ===\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m sns.set(style=\u001b[33m\"\u001b[39m\u001b[33mwhitegrid\u001b[39m\u001b[33m\"\u001b[39m, font_scale=\u001b[32m1.1\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 1) results를 DataFrame으로 변환\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m results_df = pd.DataFrame(\u001b[43mresults\u001b[49m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[INFO] 총 시도한 조합 수:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(results_df))\n\u001b[32m     16\u001b[39m results = []\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 📊 시각화 (F1 튜닝 결과용)\n",
    "# =====================================\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# 1) results를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n[INFO] 총 시도한 조합 수:\", len(results_df))\n",
    "\n",
    "results = []\n",
    "# -----------------------------\n",
    "# 1. Best config 성능 요약 막대그래프\n",
    "# -----------------------------\n",
    "best_thr = best[\"thr\"]\n",
    "best_test_f1 = best[\"test_f1\"]\n",
    "best_test_pr_auc = best[\"test_pr_auc\"]\n",
    "best_test_roc_auc = best[\"test_roc_auc\"]\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"Best OOF F1\", \"Best Test F1\", \"Test PR-AUC\", \"Test ROC-AUC\"],\n",
    "        \"value\": [best[\"oof_f1\"], best_test_f1, best_test_pr_auc, best_test_roc_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "ax = sns.barplot(data=metrics_df, x=\"metric\", y=\"value\", palette=\"viridis\", legend=False, hue='metric')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "plt.ylim(0, 1.0)\n",
    "# Best F1 모델 성능 요약 (OOF vs Test)\n",
    "plt.title(\"Best F1 Model Performance Summary (OOF vs. Test)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 주요 하이퍼파라미터 vs Test F1\n",
    "#    (max_depth, min_samples_leaf, max_features)\n",
    "# -----------------------------\n",
    "def oof_proba_for_pipe(pipe, X, y, n_splits=5, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        pipe.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "        oof[va_idx] = pipe.predict_proba(X.iloc[va_idx])[:, 1]\n",
    "    return oof\n",
    "\n",
    "\n",
    "def plot_param_effects_f1(df, score_col=\"test_f1\"):\n",
    "    params_to_plot = [\"max_depth\", \"min_samples_leaf\", \"max_features\"]\n",
    "    n = len(params_to_plot)\n",
    "    ncols = 3\n",
    "    nrows = 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 4))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for ax, param in zip(axes, params_to_plot):\n",
    "        if param not in df.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        values = df[param].unique()\n",
    "\n",
    "        # 숫자형인지 체크\n",
    "        def is_numeric(v):\n",
    "            return isinstance(v, (int, float, np.number))\n",
    "\n",
    "        if all(is_numeric(v) for v in values):\n",
    "            tmp = (\n",
    "                df.groupby(param)[score_col]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .sort_values(param)\n",
    "            )\n",
    "            sns.lineplot(data=tmp, x=param, y=score_col, marker=\"o\", ax=ax)\n",
    "        else:\n",
    "            # None이 섞여 있거나 문자열이 있을 때: 카테고리로 처리\n",
    "            tmp = df[[param, score_col]].copy()\n",
    "            tmp[param] = tmp[param].astype(str)\n",
    "            sns.boxplot(data=tmp, x=param, y=score_col, ax=ax)\n",
    "            ax.tick_params(\"x\", labelrotation=30)\n",
    "            # ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "        \n",
    "        ax.set_title(param)\n",
    "        ax.set_ylabel(score_col)\n",
    "        ax.set_xlabel(\"\")\n",
    "    # 하이퍼파라미터별 Test F1 평균\n",
    "    plt.suptitle(\"Test F1 average by hyperparameter\", y=1.05, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_param_effects_f1(results_df)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 상위 k개 조합의 Test F1 랭킹\n",
    "# -----------------------------\n",
    "k = 20\n",
    "topk = (\n",
    "    results_df.sort_values(\"test_f1\", ascending=False)\n",
    "    .head(k)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.lineplot(\n",
    "    x=topk.index + 1,\n",
    "    y=\"test_f1\",\n",
    "    data=topk,\n",
    "    marker=\"o\",\n",
    ")\n",
    "# 랜덤 그리드 상위 {k}개 조합의 Test F1\n",
    "plt.title(f\"Test F1 of the top {k} combinations of random grids\")\n",
    "plt.xlabel(\"Rank (1 = best)\")\n",
    "plt.ylabel(\"Test F1\")\n",
    "plt.xticks(range(1, k + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# [TOP-K PARAMS] 상위 F1 조합 미리보기\n",
    "print(\"\\n[TOP-K PARAMS] Preview of the top F1 combinations:\")\n",
    "display_cols = [\"test_f1\", \"test_pr_auc\", \"test_roc_auc\", \"max_depth\", \"min_samples_leaf\", \"max_features\"]\n",
    "print(topk[display_cols].head(10))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 최종 best 모델의 Threshold vs F1 곡선\n",
    "# -----------------------------\n",
    "# best params로 다시 모델 학습해 threshold 곡선을 그림\n",
    "best_params = best[\"params\"]\n",
    "best_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    sampling_strategy=best_params[\"sampling_strategy\"],\n",
    "    replacement=best_params[\"replacement\"],\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "best_pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", best_brf)])\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "best_test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, best_test_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "\n",
    "# precision_recall_curve의 thr 길이는 n-1 이라 x축 정렬을 살짝 맞춰줌\n",
    "thr_for_plot = np.r_[0.0, thr]  # 앞에 dummy 하나 붙여서 길이 맞추기\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thr_for_plot, f1s, marker=\".\")\n",
    "plt.axvline(best_thr, color=\"red\", linestyle=\"--\", label=f\"best thr = {best_thr:.3f}\")\n",
    "\n",
    "# Threshold에 따른 F1 변화 (best F1 모델)\n",
    "plt.title(\"F1 change according to threshold (best F1 model)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# F2 Score 추가 출력\n",
    "from sklearn.metrics import fbeta_score\n",
    "best_test_pred = (best_test_proba >= best_thr).astype(int)\n",
    "test_f2 = fbeta_score(y_test, best_test_pred, beta=2)\n",
    "print(f\"\\n[Test Metrics] F2-Score (beta=2, thr={best_thr:.3f}): {test_f2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# 1) 최종 best 설정\n",
    "best_params = best[\"params\"]\n",
    "train_best_thr = best[\"thr\"]   # OOF 기준으로 찾은 공식 threshold\n",
    "\n",
    "best_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    sampling_strategy=best_params[\"sampling_strategy\"],\n",
    "    replacement=best_params[\"replacement\"],\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "best_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", best_brf),\n",
    "])\n",
    "\n",
    "best_pipe.fit(X_train, y_train)\n",
    "proba_test = best_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_test)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "\n",
    "# threshold 배열 길이 맞추기\n",
    "thr_for_plot = np.r_[0.0, thr]\n",
    "\n",
    "# (1) train에서 찾은 threshold(0.741...)에 가장 가까운 F1 값 찾기\n",
    "idx_train_thr = int(np.argmin(np.abs(thr_for_plot - train_best_thr)))\n",
    "f1_at_train_thr = f1s[idx_train_thr]\n",
    "\n",
    "# (2) test 기준 F1 최고 threshold도 계산해 보고 싶다면 (선택)\n",
    "idx_test_best = int(np.nanargmax(f1s))\n",
    "test_best_thr = float(thr_for_plot[idx_test_best])\n",
    "test_best_f1 = float(f1s[idx_test_best])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thr_for_plot, f1s, marker=\".\", label=\"F1 score\")\n",
    "\n",
    "# 붉은 선: train/OOF 기준 best threshold\n",
    "plt.axvline(train_best_thr, color=\"red\", linestyle=\"--\",\n",
    "            label=f\"train best thr = {train_best_thr:.3f}\")\n",
    "plt.scatter([train_best_thr], [f1_at_train_thr],\n",
    "            color=\"red\", s=60, zorder=5)\n",
    "\n",
    "# 회색 점선 (선택): test 기준 peak\n",
    "plt.axvline(test_best_thr, color=\"gray\", linestyle=\":\",\n",
    "            label=f\"test peak thr = {test_best_thr:.3f}\")\n",
    "plt.scatter([test_best_thr], [test_best_f1],\n",
    "            color=\"gray\", s=50, zorder=5)\n",
    "\n",
    "plt.title(\"F1 Score Changes According to Threshold (Best F1 Model)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"fig_5_4_threshold_vs_f1.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Setting up Preprocessing...\n",
      "Starting Grid Search...\n",
      "[1/48] depth=None, leaf=1, max_feat=sqrt | OOF_F1=0.6890 thr=0.708 | TEST_F1=0.6667\n",
      "[2/48] depth=None, leaf=1, max_feat=0.5 | OOF_F1=0.6898 thr=0.738 | TEST_F1=0.6650\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m pipe = Pipeline([(\u001b[33m\"\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m\"\u001b[39m, preprocess), (\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, brf)])\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# OOF Prediction for validation Score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m oof = \u001b[43moof_proba_for_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m thr, oof_f1 = best_f1_threshold(y_train, oof)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Train on full train set for test eval\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36moof_proba_for_pipe\u001b[39m\u001b[34m(pipe, X, y, n_splits, seed)\u001b[39m\n\u001b[32m     70\u001b[39m     X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n\u001b[32m     71\u001b[39m     X_va = X.iloc[va_idx]\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     oof[va_idx] = pipe.predict_proba(X_va)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m oof\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:662\u001b[39m, in \u001b[36mBalancedRandomForestClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    654\u001b[39m     samplers.append(sampler)\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# that case. However, we respect any parallel_backend contexts set\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# at a higher level, since correctness does not rely on using\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m samplers_trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_local_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m samplers, trees = \u001b[38;5;28mzip\u001b[39m(*samplers_trees)\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai_basic_env/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 📊 F2 Score Optimization & Visualization (Standalone Script)\n",
    "# =====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score, roc_auc_score, fbeta_score\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuration & Data Loading\n",
    "# -----------------------------\n",
    "TRAIN_PATH = \"../../data/processed/train.csv\"\n",
    "TEST_PATH  = \"../../data/processed/test.csv\"\n",
    "TARGET_COL = \"Revenue\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df  = pd.read_csv(TEST_PATH)\n",
    "except FileNotFoundError:\n",
    "    # Fallback to absolute path or adjust as necessary if running elsewhere\n",
    "    import os\n",
    "    print(f\"Data not found at {TRAIN_PATH}. Checking current directory...\")\n",
    "    # Assuming script is run from notebooks/EomHyungEun/\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "    # Adjust logic to find data if needed\n",
    "    raise FileNotFoundError(\"Please ensure train.csv and test.csv are in ../../data/processed/\")\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].astype(int)\n",
    "X_test  = test_df.drop(columns=[TARGET_COL])\n",
    "y_test  = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Preprocessing\n",
    "# -----------------------------\n",
    "print(\"Setting up Preprocessing...\")\n",
    "num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Helper Functions\n",
    "# -----------------------------\n",
    "def oof_proba_for_pipe(pipe, X, y, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform Out-Of-Fold probability prediction.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X), dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        X_va = X.iloc[va_idx]\n",
    "        \n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        oof[va_idx] = pipe.predict_proba(X_va)[:, 1]\n",
    "    return oof\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    \"\"\"\n",
    "    Find the threshold that maximizes F1 score.\n",
    "    \"\"\"\n",
    "    prec, rec, thr = precision_recall_curve(y_true, proba)\n",
    "    f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1s))\n",
    "    best_thr = float(thr[best_idx - 1]) if best_idx > 0 else 0.5\n",
    "    return best_thr, float(f1s[best_idx])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Model Training & Grid Search\n",
    "# -----------------------------\n",
    "print(\"Starting Grid Search...\")\n",
    "\n",
    "# grid definition (Partial grid as per notebook)\n",
    "grid = []\n",
    "for max_depth in [None, 8, 12, 16]:\n",
    "    for min_leaf in [1, 2, 4, 8]:\n",
    "        for max_features in [\"sqrt\", 0.5, 0.8]:\n",
    "            grid.append({\n",
    "                \"n_estimators\": 1200,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"min_samples_leaf\": min_leaf,\n",
    "                \"min_samples_split\": 2,\n",
    "                \"max_features\": max_features,\n",
    "                \"sampling_strategy\": \"auto\",\n",
    "                \"replacement\": False,\n",
    "            })\n",
    "\n",
    "best = None\n",
    "results = []\n",
    "\n",
    "total_iter = len(grid)\n",
    "for i, params in enumerate(grid, 1):\n",
    "    brf = BalancedRandomForestClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "        min_samples_split=params[\"min_samples_split\"],\n",
    "        max_features=params[\"max_features\"],\n",
    "        sampling_strategy=params[\"sampling_strategy\"],\n",
    "        replacement=params[\"replacement\"],\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", brf)])\n",
    "\n",
    "    # OOF Prediction for validation Score\n",
    "    oof = oof_proba_for_pipe(pipe, X_train, y_train, n_splits=N_SPLITS, seed=RANDOM_STATE)\n",
    "    thr, oof_f1 = best_f1_threshold(y_train, oof)\n",
    "\n",
    "    # Train on full train set for test eval\n",
    "    pipe.fit(X_train, y_train)\n",
    "    test_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Eval on Test\n",
    "    test_pred = (test_proba >= thr).astype(int)\n",
    "    test_f1 = float(f1_score(y_test, test_pred))\n",
    "    \n",
    "    current_result = {\n",
    "        **params,\n",
    "        \"thr\": thr,\n",
    "        \"oof_f1\": oof_f1,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_pr_auc\": float(average_precision_score(y_test, test_proba)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, test_proba)),\n",
    "    }\n",
    "    results.append(current_result)\n",
    "    \n",
    "    if best is None or test_f1 > best[\"test_f1\"]:\n",
    "        best = current_result.copy()\n",
    "        best[\"params\"] = params # Store params separately to match accessing logic later\n",
    "    \n",
    "    print(f\"[{i}/{total_iter}] depth={params['max_depth']}, leaf={params['min_samples_leaf']}, \"\n",
    "          f\"max_feat={params['max_features']} | OOF_F1={oof_f1:.4f} thr={thr:.3f} | TEST_F1={test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== BEST CONFIG (by TEST F1) ===\")\n",
    "print(best)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Visualization & F2 Score\n",
    "# -----------------------------\n",
    "print(\"\\nGenerating Visualizations...\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n[INFO] Total combinations trained:\", len(results_df))\n",
    "\n",
    "# --- Plot 1: Best config performance ---\n",
    "best_thr = best[\"thr\"]\n",
    "best_test_f1 = best[\"test_f1\"]\n",
    "best_test_pr_auc = best[\"test_pr_auc\"]\n",
    "best_test_roc_auc = best[\"test_roc_auc\"]\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\"Best OOF F1\", \"Best Test F1\", \"Test PR-AUC\", \"Test ROC-AUC\"],\n",
    "        \"value\": [best[\"oof_f1\"], best_test_f1, best_test_pr_auc, best_test_roc_auc],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "ax = sns.barplot(data=metrics_df, x=\"metric\", y=\"value\", palette=\"viridis\", legend=False, hue='metric')\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"Best F1 Model Performance Summary (OOF vs. Test)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Hyperparameter Effects ---\n",
    "def plot_param_effects_f1(df, score_col=\"test_f1\"):\n",
    "    params_to_plot = [\"max_depth\", \"min_samples_leaf\", \"max_features\"]\n",
    "    n = len(params_to_plot)\n",
    "    ncols = 3\n",
    "    nrows = 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 4))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for ax, param in zip(axes, params_to_plot):\n",
    "        if param not in df.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        values = df[param].unique()\n",
    "        def is_numeric(v):\n",
    "            return isinstance(v, (int, float, np.number))\n",
    "\n",
    "        if all(is_numeric(v) for v in values):\n",
    "            tmp = df.groupby(param)[score_col].mean().reset_index().sort_values(param)\n",
    "            sns.lineplot(data=tmp, x=param, y=score_col, marker=\"o\", ax=ax)\n",
    "        else:\n",
    "            tmp = df[[param, score_col]].copy()\n",
    "            tmp[param] = tmp[param].astype(str)\n",
    "            sns.boxplot(data=tmp, x=param, y=score_col, ax=ax)\n",
    "            ax.tick_params(\"x\", labelrotation=30)\n",
    "        \n",
    "        ax.set_title(param)\n",
    "        ax.set_ylabel(score_col)\n",
    "        ax.set_xlabel(\"\")\n",
    "    plt.suptitle(\"Test F1 average by hyperparameter\", y=1.05, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_param_effects_f1(results_df)\n",
    "\n",
    "# --- Plot 3: Top K Ranking ---\n",
    "k = 20\n",
    "topk = results_df.sort_values(\"test_f1\", ascending=False).head(k).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.lineplot(x=topk.index + 1, y=\"test_f1\", data=topk, marker=\"o\")\n",
    "plt.title(f\"Test F1 of the top {k} combinations of random grids\")\n",
    "plt.xlabel(\"Rank (1 = best)\")\n",
    "plt.ylabel(\"Test F1\")\n",
    "plt.xticks(range(1, k + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TOP-K PARAMS] Preview of the top F1 combinations:\")\n",
    "display_cols = [\"test_f1\", \"test_pr_auc\", \"test_roc_auc\", \"max_depth\", \"min_samples_leaf\", \"max_features\"]\n",
    "print(topk[display_cols].head(10))\n",
    "\n",
    "# --- Plot 4: Threshold vs F1 Curve for Best Model ---\n",
    "best_params = best[\"params\"]\n",
    "best_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    sampling_strategy=best_params[\"sampling_strategy\"],\n",
    "    replacement=best_params[\"replacement\"],\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "best_pipe = Pipeline([(\"preprocess\", preprocess), (\"model\", best_brf)])\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "best_test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, best_test_proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "\n",
    "thr_for_plot = np.r_[0.0, thr]  # align length\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(thr_for_plot, f1s, marker=\".\")\n",
    "plt.axvline(best_thr, color=\"red\", linestyle=\"--\", label=f\"best thr = {best_thr:.3f}\")\n",
    "plt.title(\"F1 change according to threshold (best F1 model)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. F2 Score Calculation\n",
    "# -----------------------------\n",
    "best_test_pred = (best_test_proba >= best_thr).astype(int)\n",
    "test_f2 = fbeta_score(y_test, best_test_pred, beta=2)\n",
    "print(f\"\\n[Test Metrics] F2-Score (beta=2, thr={best_thr:.3f}): {test_f2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
